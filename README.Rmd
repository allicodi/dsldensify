---
output:
  rmarkdown::github_document
always_allow_html: yes
bibliography: "inst/REFERENCES.bib"
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%",
  fig.path = "README-"
)
```


# R/`dsldensify`

> **Discrete Super Learner Conditional Density Estimation**

**Author:**  
David Benkeser (Emory University)

---

## What is `dsldensify`?

`dsldensify` is an R package for **cross-validated conditional density estimation**
of a continuous variable \(A \mid W\), using a **discrete Super Learner–style
model selection framework**.

The package is designed for settings where:

- the full conditional distribution of \(A \mid W\) is required (not just a mean),
- flexible machine-learning estimators are desired,
- and model selection is performed by **cross-validated negative log-density risk**.

The project is under active development and should currently be viewed as
**research-grade software** rather than a fully stable production library.

---

## Modeling strategies supported

At a high level, `dsldensify` supports **two complementary approaches** to
conditional density estimation under a common cross-validation interface.

### 1. Hazard-based (discretized) density estimation

In this approach, the support of \(A\) is discretized into bins, and the
conditional density is estimated by modeling **discrete-time hazards**:

- \(A\) is binned using either equal-range or equal-mass binning.
- Long-format data are constructed with one row per observation–bin pair.
- Binary regression models estimate the hazard of falling into each bin.
- Estimated hazards are converted into a continuous density.

This strategy allows the use of a wide range of binary learners (e.g.,
logistic regression, penalized GLMs, random forests, gradient boosting).

### 2. Direct conditional density models

Direct models estimate \(f(A \mid W)\) without discretization, for example:

- homoskedastic Gaussian regression,
- flexible GAMLSS models with covariate-dependent parameters.

These models are evaluated using the same cross-validated log-density loss as
hazard-based learners and can compete directly with them in model selection.

---

## Installation

The development version of `dsldensify` can be installed from GitHub:

```r
remotes::install_github("benkeser/dsldensify")
```

---

## Example

Below is a simple example using synthetic data. We combine:

- a hazard-based learner using gradient boosting, and
- a direct Gaussian conditional density model.

A simple example illustrates how `haldensify` may be used to train a discrete super learner on a small data set. In this example, we leverage a hazard-based density learner that uses `xgboost` and a simple, direct density estimate based on a homoscedastic, Gaussian linear model.

```{r example-fit}
library(data.table)
library(dsldensify)

# generate data
set.seed(1)

n <- 200
W <- data.table(
  x1 = rnorm(n),
  x2 = rnorm(n)
)

A <- 0.7 * W$x1 - 0.3 * W$x2 + rnorm(n)

# define xgboost hazard learner
# see ?make_xgboost_runner for details
hazard_learners <- list(
  xgb = make_xgboost_runner(
    rhs_list = list(~ x1 + x2 + bin_id),
    max_depth_grid = c(2, 4),
    eta_grid = c(0.1, 0.05),
    nrounds_max = 100,
    early_stopping_rounds = 10
  )
)

# define Gaussian linear model direct density learner
# see ?make_gaussian_homosked_runner for details
direct_learners <- list(
  gaussian = make_gaussian_homosked_runner(
    rhs_list = list(~ x1 + x2)
  )
)

fit <- dsldensify(
  A = A,
  W = W,
  hazard_learners = hazard_learners,
  direct_learners = direct_learners,
  grid_type = c("equal_mass"),
  n_bins = c(5, 10),
  cv_folds = 3
)

fit

```

---

## Visualizing fitted densities 

We can visualize the fit by plotting the conditional density estimates for fixed covariate profiles.

```{r example-plot1, out.width = "80%", fig.alt = ""}
W_grid <- data.table(
  x1 = c(0, 1),
  x2 = c(0, 0)
)

plot(fit, W_grid, mode = "conditional")
```

We can also sanity check the fit by comparing the marginal density estimate implied by the conditional model to a simple histogram. 

```{r example-plot2, out.width = "80%", fig.alt = ""}
hist(
  A, freq = FALSE, breaks = 30, col = "gray", border = "white",
  main = "Marginal distribution of A"
)
plot(fit, W_grid, mode = "marginal", add = TRUE, plot_args = list(lwd = 2))
```

---

## Prediction 

We can obtain conditional density estimates from the trained model on arbitrary new data.

```{r example-predict}
A_new <- seq(-3, 3, length.out = 100)
W_new <- data.table(x1 = 0, x2 = 0)

dens <- predict(fit, A = A_new, W = W_new)
head(dens)
```

Cross-validated prediction is also supported (e.g., for DML/TMLE workflows); see the function documentation for details.

---

## Sampling from the fitted conditional density

In addition to evaluating densities, `dsldensify` can generate draws from the fitted conditional distribution \(\hat f(\cdot \mid W)\). 

```{r}
# draw 5 samples of A for each row of W_grid
A_samp <- rsample(fit, W = W_grid, n_samp = 5, type = "full", seed = 123)

A_samp
```

A quick sanity check is to compare the sampled values to the fitted conditional density curve at the same covariate profile.

```{r example-sample, out.width = "80%", fig.alt = ""}
# overlay samples on the fitted conditional density for W_new = (0, 0)
W_new <- data.table(x1 = 0, x2 = 0)
A_samp0 <- rsample(fit, W = W_new, n_samp = 2000, type = "full", seed = 1)

hist(
  A_samp0[1, ], freq = FALSE, breaks = 30, col = "gray", border = "white",
  main = "Samples from estimated density + fitted density curve"
)

plot(
  fit, W_new, mode = "conditional", add = TRUE, plot_args = list(lwd = 2)
)
```

Cross-validated sampling is also supported; see the function documentation for details.

---

## Supported learners

All estimators in `dsldensify` are implemented as **runners**: adapters that expose a common interface for fitting, prediction, tuning, and cross-validation. This design allows hazard-based and direct density learners to compete under the same cross-validated log-density risk and allows for building out additional learners under this same framework.

| Category       | Learner                                                         | Runner function                   |
| -------------- | --------------------------------------------------------------- | --------------------------------- |
| Hazard-based   | Logistic regression (`stats::glm`)                              | `make_glm_runner()`               |
| Hazard-based   | Penalized logistic (`glmnet`)                                   | `make_glmnet_runner()`            |
| Hazard-based   | Gradient boosting (`xgboost`)                                   | `make_xgboost_runner()`           |
| Hazard-based   | Random forest (`ranger`)                                        | `make_rf_runner()`                |
| Direct density | Gaussian linear model, homoskedastic                            | `make_gaussian_homosked_runner()` |
| Direct density | GAMLSS (`gamlss`)                                               | `make_gamlss_runner()`            |
| Direct density | Gaussian mixture of experts (EM, optional gating)               | `make_gmm_runner()`               |
| Direct density | Location–scale model with residual KDE                          | `make_locscale_kde_runner()`      |
| Direct density | Mixture of location–scale residual KDE experts                  | `make_mix_locscale_kde_runner()`  |
| Quantile inversion | Quantile-function–based density (quantile regression) | `make_quantreg_runner()`          |


---

## Status

The API is still evolving and internal representations may change.

Unit tests are minimal for the time being.

Despite this, the package already supports:

- multiple learner backends,
- multiple binning strategies,
- attempts at memory-efficient cross-validation,

A package vignette is (probably) coming soon.

---

## Issues

If you encounter any bugs or have any specific feature requests, please [file an issue](https://github.com/benkeser/dsldensify/issues).

---

## Contributions

Contributions are welcome. Please see [contribution guidelines](https://github.com/benkeser/dsldensify/blob/master/CONTRIBUTING.md) prior to submitting a pull request.

---

<!-- ## Citation -->

## Related

* [R/`haldensify`](https://github.com/nhejazi/haldensify) -- Earlier work that inspired parts of the hazard-based density machinery.

---

## License

&copy; 2019-2026 David Benkeser

The contents of this repository are distributed under the MIT license.