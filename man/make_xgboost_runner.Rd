% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/runner_xgboost.R
\name{make_xgboost_runner}
\alias{make_xgboost_runner}
\title{Create an xgboost runner for discrete-time hazard modeling}
\usage{
make_xgboost_runner(
  rhs_list,
  max_depth_grid = c(2L, 4L),
  eta_grid = c(0.05, 0.1),
  min_child_weight_grid = c(1, 5),
  subsample_grid = 0.8,
  colsample_bytree_grid = 0.8,
  gamma_grid = 0,
  reg_lambda_grid = 1,
  reg_alpha_grid = 0,
  nrounds_max = 2000L,
  early_stopping_rounds = 30L,
  valid_frac = 0.2,
  valid_by_id = TRUE,
  id_col = "obs_id",
  bin_var = "bin_id",
  require_bin_id = TRUE,
  use_weights_col = TRUE,
  objective = "binary:logistic",
  eval_metric = "logloss",
  verbose = 0L,
  nthread = 0L,
  eps = 1e-08,
  strip_fit = TRUE,
  strip_method = c("none", "best_iter_refit")
)
}
\arguments{
\item{rhs_list}{A list of one-sided RHS formulas, such as
\code{list(~ bin_id + W1 + W2, ~ bin_id + W1 + W2 + W3)}. Variable names are
extracted using \code{all.vars()} and used solely for column selection.}

\item{max_depth_grid}{Integer vector of tree depths to tune over.}

\item{eta_grid}{Numeric vector of learning rates.}

\item{min_child_weight_grid}{Numeric vector controlling minimum node weight.}

\item{subsample_grid}{Numeric vector of row subsampling fractions.}

\item{colsample_bytree_grid}{Numeric vector of column subsampling fractions.}

\item{gamma_grid}{Numeric vector of minimum split-loss reduction values.}

\item{reg_lambda_grid}{Numeric vector of L2 regularization parameters.}

\item{reg_alpha_grid}{Numeric vector of L1 regularization parameters.}

\item{nrounds_max}{Integer maximum number of boosting iterations.}

\item{early_stopping_rounds}{Integer number of rounds with no improvement
before early stopping. Set to \code{NULL} to disable early stopping.}

\item{valid_frac}{Fraction of training data used for internal validation
when early stopping is enabled.}

\item{valid_by_id}{Logical. If \code{TRUE}, validation splits are constructed
at the subject level using \code{obs_id}.}

\item{id_col}{Name of the subject identifier column used when
\code{valid_by_id = TRUE}.}

\item{bin_var}{Name of the time-bin variable. Automatically added to RHS
specifications when \code{require_bin_id = TRUE}.}

\item{require_bin_id}{Logical. If \code{TRUE} (default), ensure \code{bin_id}
is included in all RHS feature sets.}

\item{use_weights_col}{Logical. If \code{TRUE} and a column named \code{wts}
(or \code{weights}) is present, it is passed to \code{xgboost} as case weights.}

\item{objective}{Character string passed to \code{xgboost::xgb.train()}.
Defaults to \code{"binary:logistic"}.}

\item{eval_metric}{Evaluation metric passed to \code{xgboost}. Defaults to
\code{"logloss"}.}

\item{verbose}{Integer verbosity level passed to \code{xgboost}.}

\item{nthread}{Integer number of threads used by \code{xgboost}.}

\item{eps}{Numeric tolerance used to clip predicted hazards away from
\code{0} and \code{1}.}

\item{strip_fit}{Logical. If \code{TRUE}, store a lightweight representation
of each fitted model.}

\item{strip_method}{Method used when \code{strip_fit = TRUE}. Currently
supports \code{"none"} and \code{"best_iter_refit"}.}
}
\value{
A named list (runner) with elements:
\describe{
  \item{method}{Character string \code{"xgboost"}.}
  \item{tune_grid}{Data frame describing the tuning grid, including
        \code{.tune} and hyperparameter columns.}
  \item{fit}{Function \code{fit(train_set, ...)} returning a fit bundle.}
  \item{predict}{Function \code{predict(fit_bundle, newdata, ...)} returning
        an \code{n_long x K} matrix of hazard predictions.}
  \item{fit_one}{Function \code{fit_one(train_set, tune, ...)} fitting only
        the selected tuning index.}
  \item{sample}{Function \code{sample(fit_bundle, newdata, n_samp, ...)}
        drawing samples from the implied conditional density (assumes
        \code{length(fit_bundle$fits)==1}).}
}
}
\description{
Constructs a **runner** (learner adapter) compatible with the
\code{run_grid_setting()} / \code{summarize_and_select()} workflow used in
\code{dsl_densify}. The runner fits gradient-boosted decision tree models via
\code{xgboost::xgb.train()} on long-format discrete-time hazard data
(binary outcome \code{in_bin}), and supports a tuning grid over:
\itemize{
  \item multiple RHS feature specifications (\code{rhs_list}),
  \item tree depth and learning-rate parameters,
  \item node- and split-regularization parameters.
}

The fitted models estimate per-bin discrete-time hazards
\eqn{P(T \in \text{bin}_j \mid T \ge \text{bin}_j, W)} using a logistic loss,
which is equivalent to maximizing the discrete-time hazard likelihood under
the long-data construction used by \code{dsl_densify}.
}
\details{
## Data requirements
The runner expects \code{train_set} and \code{newdata} as
\code{data.table}s in the **long hazard format** produced by
\code{format_long_hazards()}, including:
\itemize{
  \item a binary outcome column \code{in_bin},
  \item a time-bin column \code{bin_id},
  \item covariates referenced in \code{rhs_list},
  \item an optional weight column \code{wts}.
}

\code{newdata} passed to \code{sample()} must additionally include
\code{bin_lower} and \code{bin_upper}.

## Model selection
Each row of \code{tune_grid} corresponds to a distinct fitted model, so no
\code{select_fit()} method is required. The \code{.tune} index uniquely
identifies the fitted xgboost model.
}
\section{RHS specifications (column selection only)}{

The \code{rhs_list} argument follows the same *interface* as the GLM and
GLMNET runners, but is interpreted more restrictively:
\itemize{
  \item RHS formulas are used **only to select columns** from the data.
  \item No transformations, interactions, or spline terms are evaluated.
  \item The variable names extracted via \code{all.vars()} define the feature set.
}

This design keeps the runner lightweight and delegates all feature
engineering (encoding, interactions, splines, etc.) to upstream code.

By default, \code{bin_id} is automatically included in each RHS specification
unless \code{require_bin_id = FALSE}.
}

\section{Numeric-only requirement}{

This runner operates directly on numeric feature matrices passed to
\code{xgboost}. All columns referenced in \code{rhs_list} **must already be
numeric** (including \code{bin_id} and all covariates in \code{W}).
Factors, characters, and ordered factors are not supported and should be
encoded upstream. No coercion is performed internally.
}

\section{Tuning grid and prediction layout}{

The internal \code{tune_grid} is constructed using \code{expand.grid()} with
**RHS varying first**, followed by tree hyperparameters:
\itemize{
  \item \code{rhs},
  \item \code{max_depth},
  \item \code{eta},
  \item \code{min_child_weight},
  \item \code{subsample},
  \item \code{colsample_bytree},
  \item \code{gamma},
  \item \code{reg_lambda},
  \item \code{reg_alpha}.
}

Each row of \code{tune_grid} corresponds to **exactly one fitted xgboost model**.
During cross-validation, \code{predict()} returns an
\code{n_long x K} matrix of predicted hazards, where
\code{K = nrow(tune_grid)}, with columns aligned to \code{.tune}.

Internally, \code{predict()} delegates to a local helper \code{predict_hazards()}
to avoid duplicated scoring logic and to ensure sampling and prediction use
the same hazard predictions.
}

\section{Sampling from the fitted hazard model}{

The runner provides a \code{sample()} method that generates draws
\eqn{A^* \sim \hat f(\cdot \mid W)} from the implied conditional density under
the discrete-time hazard representation.

Sampling assumes the \code{fit_bundle} contains **exactly one** tuned fit
(i.e., \code{length(fit_bundle$fits) == 1}). This is the intended usage after
model selection (e.g., after applying \code{select_fit_tune()} or fitting the
selected tuning index via \code{fit_one()}).

IMPORTANT: This runner's \code{sample()} expects \code{newdata} in **long hazard
format**. Expansion of wide \code{W} to long (repeating rows across all bins,
attaching \code{bin_lower}/\code{bin_upper}) is handled upstream by the
hazard-grid orchestration utilities in \code{dsldensify}.
}

\section{Early stopping}{

If \code{early_stopping_rounds} is not \code{NULL}, each model is trained with
early stopping using an internal validation split drawn from the training data.
Validation splits may be constructed at the subject level (via \code{obs_id})
or at the row level.

Early stopping uses standard logistic log loss on the hazard outcome
\code{in_bin}. No custom objective or evaluation function is required, as this
loss is already equivalent to the discrete-time hazard likelihood.
}

\section{Lightweight fit objects}{

When \code{strip_fit = TRUE}, fitted models are reduced to a minimal
representation sufficient for prediction:
\itemize{
  \item the fitted \code{xgboost} booster (optionally refit to the
        early-stopping iteration),
  \item the selected feature column names.
}

If \code{strip_method = "best_iter_refit"}, models trained with early stopping
are refit to exactly the selected number of boosting iterations, discarding
unused trees and substantially reducing memory usage.
}

\examples{
rhs_list <- list(
  ~ bin_id + W1 + W2,
  ~ bin_id + W1 + W2 + W3
)

runner <- make_xgboost_runner(
  rhs_list = rhs_list,
  max_depth_grid = c(2L, 4L),
  eta_grid = c(0.05, 0.1),
  strip_fit = TRUE
)

}
