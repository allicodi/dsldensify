% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/runner_hurdle_xgboost.R
\name{make_xgboost_hurdle_runner}
\alias{make_xgboost_hurdle_runner}
\title{Create an xgboost runner for hurdle probability modeling}
\usage{
make_xgboost_hurdle_runner(
  rhs_list,
  max_depth_grid = c(2L, 4L),
  eta_grid = c(0.05, 0.1),
  min_child_weight_grid = c(1, 5),
  subsample_grid = 0.8,
  colsample_bytree_grid = 0.8,
  gamma_grid = 0,
  reg_lambda_grid = 1,
  reg_alpha_grid = 0,
  nrounds_max = 2000L,
  early_stopping_rounds = 30L,
  valid_frac = 0.2,
  valid_by_id = FALSE,
  id_col = "obs_id",
  use_weights_col = TRUE,
  weights_col = "wts",
  objective = "binary:logistic",
  eval_metric = "logloss",
  verbose = 0L,
  nthread = 0L,
  eps = 1e-08,
  strip_fit = TRUE,
  strip_method = c("none", "best_iter_refit"),
  seed = NULL
)
}
\arguments{
\item{rhs_list}{A list of RHS specifications, either one-sided formulas
(for example, ~ W1 + W2) or character strings (for example, "W1 + W2").}

\item{max_depth_grid}{Integer vector of maximum tree depths.}

\item{eta_grid}{Numeric vector of learning rates.}

\item{min_child_weight_grid}{Numeric vector of minimum child weights.}

\item{subsample_grid}{Numeric vector of subsample proportions.}

\item{colsample_bytree_grid}{Numeric vector of column subsample proportions.}

\item{gamma_grid}{Numeric vector of minimum loss reduction.}

\item{reg_lambda_grid}{Numeric vector of L2 regularization strengths.}

\item{reg_alpha_grid}{Numeric vector of L1 regularization strengths.}

\item{nrounds_max}{Integer maximum number of boosting rounds.}

\item{early_stopping_rounds}{Integer number of early-stopping rounds; set to
NULL to disable early stopping.}

\item{valid_frac}{Fraction of observations (or ids) used for validation when
early stopping is enabled.}

\item{valid_by_id}{Logical. If TRUE, validation split is made by id_col.}

\item{id_col}{Column name used when valid_by_id = TRUE.}

\item{use_weights_col}{Logical. If TRUE and weights_col is present, weights
are passed to xgboost::xgboost() via the weight argument.}

\item{weights_col}{Name of the weights column in the wide data.}

\item{objective}{Objective passed to xgboost::xgboost() (defaults to
binary:logistic).}

\item{eval_metric}{Evaluation metric passed to xgboost::xgboost().}

\item{verbose}{Verbosity level for xgboost::xgboost().}

\item{nthread}{Number of threads used by xgboost::xgboost().}

\item{eps}{Numeric stability parameter for clipping probabilities.}

\item{strip_fit}{Logical. If TRUE, attempt to reduce stored fit size.}

\item{strip_method}{Method for stripping fit objects.}

\item{seed}{Optional integer seed for deterministic fitting across tuning
rows.}
}
\value{
A named list (runner) with the following elements:
  method: Character string "hurdle_xgboost".
  tune_grid: Data frame describing the tuning grid.
  fit: Function fit(train_set, ...) returning a fit bundle.
  fit_one: Function fit_one(train_set, tune, ...) fitting only the selected
    tuning index.
  logpi: Function logpi(fit_bundle, newdata, ...) returning log probabilities.
  log_density: Function log_density(fit_bundle, newdata, ...) returning
    Bernoulli negative log-likelihoods.
  sample: Function sample(fit_bundle, newdata, n_samp, ...) drawing hurdle
    indicators (assumes K = 1).

Data requirements

The runner expects train_set and newdata as wide data containing:
  - a binary outcome column in_hurdle,
  - covariates referenced in rhs_list,
  - an optional weight column wts (or weights_col).
}
\description{
Constructs a runner (learner adapter) for modeling the hurdle probability
\eqn{\pi(W) = P(A = a_0 \mid W)} using gradient boosting via
xgboost::xgboost(). The runner is compatible with the hurdle workflow in
dsldensify, where the hurdle component is fit on wide data with binary
outcome \code{in_hurdle}.
}
\details{
Tuning grid

Tuning is performed over a grid defined by:
  - rhs_list (RHS varies slowest),
  - tree depth, learning rate, and regularization parameters,
  - subsampling and column subsampling parameters.

Numeric-only requirement

This runner is intended for use with numeric predictors only. All columns
referenced by rhs_list must already be numeric. Factors, characters, and
ordered factors are not supported and should be encoded upstream.

Early stopping

If early_stopping_rounds is provided, an internal validation split is
created within each fold. When valid_by_id is TRUE, the split is performed
by id_col; otherwise it is performed at the row level.
}
\examples{
rhs_list <- list(~ W1 + W2)

runner <- make_xgboost_hurdle_runner(
  rhs_list = rhs_list,
  max_depth_grid = c(2L, 4L),
  eta_grid = c(0.05, 0.1)
)

}
