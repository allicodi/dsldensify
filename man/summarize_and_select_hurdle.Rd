% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{summarize_and_select_hurdle}
\alias{summarize_and_select_hurdle}
\title{Summarize and select the best joint hurdle density model by cross-validated risk}
\usage{
summarize_and_select_hurdle(
  pos_select_out,
  hurdle_out,
  hazard_learners = NULL,
  direct_learners = NULL,
  hurdle_learners,
  weighted = TRUE,
  eps = 1e-15
)
}
\arguments{
\item{pos_select_out}{List of positive-part selection outputs. Each element
corresponds to one positive-part grid setting (hazard binning setting or
direct setting) and must contain \code{$grid_type}, \code{$n_bins}, and
\code{$cv_out}. Each \code{cv_out[[v]]} must contain \code{$learners} with
per-learner \code{$loss} for the positive-part density evaluation.}

\item{hurdle_out}{Output from \code{run_hurdle_setting()}. Must contain
\code{$cv_out} with one element per fold. Each fold object must contain:
\describe{
  \item{\code{fold}}{Fold index.}
  \item{\code{fold_weight}}{Fold weight used for weighted aggregation.}
  \item{\code{validation_ids_full}}{Full-data row indices for the fold
    validation set, in the ordering used by \code{valid_in_hurdle}.}
  \item{\code{valid_in_hurdle}}{Integer 0/1 vector indicating whether each
    validation observation equals the hurdle point.}
  \item{\code{learners}}{Named list of hurdle learner fold outputs, each
    containing either \code{$logpi} or \code{$pi}.}
}}

\item{hazard_learners}{Optional named list of hazard-based runner objects used
as positive-part density candidates.}

\item{direct_learners}{Optional named list of direct density runner objects
used as positive-part density candidates.}

\item{hurdle_learners}{Named list of hurdle runner objects. Names must match
the learner keys stored in \code{hurdle_out$cv_out[[v]]$learners}.}

\item{weighted}{Logical. If \code{TRUE}, aggregate fold losses using
\code{fold_weight} from fold objects. If \code{FALSE}, aggregate by simple
averaging across folds.}

\item{eps}{Small positive constant used to bound hurdle probabilities away
from \code{0} and \code{1} when converting \code{logpi} to \code{pi}.}
}
\value{
A named list with elements:
\describe{
  \item{\code{summary_all}}{A \code{data.table} with one row per joint candidate
    and columns \code{grid_type}, \code{n_bins}, \code{hurdle_learner},
    \code{hurdle_tune}, \code{pos_learner}, \code{pos_tune}, \code{cv_risk},
    and \code{V} (number of folds contributing). Sorted by \code{cv_risk}.}
  \item{\code{summary_by_learner}}{Reserved for compatibility; currently \code{NULL}.}
  \item{\code{best}}{A single-row \code{data.table} giving the minimizing joint
    candidate.}
}
}
\description{
Aggregates fold-level losses from (i) a set of positive-part density candidates
and (ii) a set of hurdle probability candidates, and selects the best joint
hurdle density model by minimizing cross-validated negative log-likelihood risk.

The hurdle density model has a point mass at \code{hurdle_point} (handled by
\code{run_hurdle_setting()}) and a positive-part conditional density
\eqn{f_+(a \mid w)} (handled by \code{run_grid_setting()} and/or
\code{run_direct_setting()} applied on the subset \code{A != hurdle_point}).

This function constructs, for each fold, a long table of joint candidates over:
\enumerate{
  \item hurdle learner name and hurdle tuning index,
  \item positive-part learner name and positive-part tuning index,
  \item positive-part grid specification (hazard binning settings or direct).
}
Fold-level joint losses are then aggregated (optionally weighted by fold
weights) to produce a cross-validated risk for each joint candidate.
}
\details{
Let \eqn{a_0} denote the hurdle point. The fitted hurdle model is a two-part
mixture:
\deqn{
  f(a \mid w) =
  \pi(w)\,\mathbb{I}\{a = a_0\} +
  \left\{1 - \pi(w)\right\} f_+(a \mid w)\,\mathbb{I}\{a \ne a_0\},
}
where \eqn{\pi(w) = P(A = a_0 \mid W = w)} is estimated by a hurdle learner and
\eqn{f_+(a \mid w)} is estimated by a positive-part density learner fit on
observations with \code{A != a_0}.

On a fold validation set, the joint negative log-likelihood contribution is:
\itemize{
  \item for hurdle observations (\eqn{A = a_0}): \eqn{-\log \pi(W)},
  \item for positive observations (\eqn{A \ne a_0}): \eqn{-\log\{1-\pi(W)\} - \log f_+(A \mid W)}.
}
This function assumes that the positive-part fold output \code{$loss} is
already the negative log-density for \eqn{f_+} evaluated on the positive
validation subset.
}
\seealso{
\code{\link{summarize_and_select}} \code{\link{dsldensify}}
}
