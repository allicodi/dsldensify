% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{select_fit_tune}
\alias{select_fit_tune}
\title{Select a single tuning value from a fitted learner bundle}
\usage{
select_fit_tune(runner, fit_bundle, tune)
}
\arguments{
\item{runner}{A learner runner object. If it defines a function
\code{runner$select_fit}, that function is used to perform the tuning
selection.}

\item{fit_bundle}{An object returned by \code{runner$fit()}, potentially
containing fits for multiple tuning values.}

\item{tune}{Integer index of the selected tuning configuration (1-based).}
}
\value{
A fit bundle corresponding to the selected tuning configuration. The returned
object is suitable for passing directly to \code{runner$predict()}.
}
\description{
Extracts the fit corresponding to a chosen tuning index from a learner-specific
fit bundle. This function provides a generic mechanism for reducing a
multi-tuning fit object (e.g., multiple formulas, lambdas, or hyperparameter
settings) to the single fit (e.g., selected by cross-validation).

The behavior is learner-dependent:
\itemize{
  \item If the learner runner defines a \code{select_fit()} method, that method
  is used to extract the selected tuning in a learner-specific way.
  \item Otherwise, a default rule is applied assuming \code{fit_bundle$fits} is
  a list of fits indexed by tuning parameter. In this case, only the element
  corresponding to \code{tune} is retained.
  \item If neither condition applies, the fit bundle is returned unchanged,
  which is appropriate when the learner has no tuning parameter or when tuning
  selection is irrelevant.
}
}
\details{
This helper is intended to be used after model selection, for example when:
\itemize{
  \item Storing only the selected fit from each cross-validation fold
  \item Refitting the selected model on the full data and discarding unused
    tuning configurations
}

By delegating to \code{runner$select_fit()} when available, this function allows
different learners (e.g., \code{glm}, \code{glmnet}, tree-based models) to store
and reduce their fitted objects in a way that is natural for the underlying
fitting procedure, while keeping the higher-level pipeline generic.
}
\examples{
\dontrun{
# Default behavior with a list-of-fits bundle
fit_bundle <- list(fits = list(fit1, fit2, fit3))
fit_sel <- select_fit_tune(runner, fit_bundle, tune = 2)
# fit_sel$fits contains only fit2

# Learner-specific selection
runner$select_fit <- function(fit_bundle, tune) {
  fit_bundle$lambda <- fit_bundle$lambda[tune]
  fit_bundle
}
fit_sel <- select_fit_tune(runner, fit_bundle, tune = 5)
}

}
