% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dsldensify.R
\name{dsldensify}
\alias{dsldensify}
\title{Discrete super learner for conditional density estimation}
\usage{
dsldensify(
  A,
  W,
  hazard_learners,
  direct_learners,
  wts = rep(1, length(A)),
  grid_type = c("equal_range", "equal_mass"),
  n_bins = round(c(0.5, 1, 1.5, 2) * sqrt(length(A))),
  cv_folds = 5L,
  return_cv_fits = FALSE,
  refit_dsl_full_data = TRUE,
  ...
)
}
\arguments{
\item{A}{Numeric vector of length \code{n} containing observed outcomes.}

\item{W}{Covariates used to condition the density. May be a vector, matrix,
\code{data.frame}, or \code{data.table}.}

\item{hazard_learners}{Named list of hazard-based runner objects. Each runner
must support fitting on long-format hazard data and prediction of hazards.}

\item{direct_learners}{Named list of direct conditional density runner objects.
Each runner must support fitting on wide data and evaluation of the
conditional log-density.}

\item{wts}{Optional numeric vector of length \code{n} giving observation
weights.}

\item{grid_type}{Character vector specifying binning strategies for hazard
learners. Allowed values are \code{"equal_range"} and \code{"equal_mass"}.}

\item{n_bins}{Integer vector giving numbers of bins to consider for hazard
learners.}

\item{cv_folds}{Either a single integer specifying the number of v-folds, or a
fold object returned by \code{origami::make_folds()}.}

\item{return_cv_fits}{Logical; whether to retain fold-specific fits for the
selected learner.}

\item{refit_dsl_full_data}{Logical; whether to refit the selected learner on
the full dataset after selection.}

\item{...}{Additional arguments passed to internal fitting routines and to
learner-specific runner methods.}
}
\value{
An object of class \code{"dsldensify"} containing the selected learner,
  tuning parameters, cross-validation summaries, and fitted models.
}
\description{
Fits and selects among candidate conditional density estimators for a
continuous outcome \code{A} given covariates \code{W}, using cross-validated
negative log-density risk. Candidate estimators may include hazard-based
(discretized) density learners and direct conditional density learners.
}
\details{
Hazard-based learners approximate the conditional density by discretizing the
support of \code{A} and modeling discrete-time hazards. Direct learners model
the conditional density of \code{A | W} without discretization.


Hazard-based density estimation relies on the identity
\deqn{
  f(a \mid w) = \lambda_j(w)
  \prod_{k < j} \left\{1 - \lambda_k(w)\right\} \Big/ \Delta_j,
}
where \eqn{\lambda_j(w)} is the discrete hazard of \eqn{A} falling in bin
\eqn{j} given \eqn{W = w}, and \eqn{\Delta_j} is the bin width. By modeling the
hazard with flexible binary regression learners and combining bins via the
above factorization, one obtains a valid conditional density estimator.

Cross-validated negative log-likelihood provides a proper scoring rule for
comparing both hazard-based and direct density learners on equal footing.
}
\examples{
set.seed(1)

n <- 100
W <- data.frame(
  x1 = rnorm(n),
  x2 = rnorm(n)
)
A <- 0.5 * W$x1 - 0.3 * W$x2 + rnorm(n)

# Simple Gaussian direct density learner
gaussian_runner <- make_gaussian_homosked_runner(
  rhs_list = "~ x1 + x2"
)

fit <- dsldensify(
  A = A,
  W = W,
  hazard_learners = NULL,
  direct_learners = list(gaussian = gaussian_runner),
  cv_folds = 3
)

# Predict conditional density at new covariate values
new_W <- data.frame(x1 = c(0, 1), x2 = c(0, 0))
dens <- predict(fit, A = c(0, 0), W = new_W)
dens

}
\seealso{
\code{\link{predict.dsldensify}}
}
