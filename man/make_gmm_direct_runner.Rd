% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/runner_direct_gmm.R
\name{make_gmm_direct_runner}
\alias{make_gmm_direct_runner}
\title{Create a Gaussian mixture-of-experts runner for direct conditional density estimation}
\usage{
make_gmm_direct_runner(
  rhs_list,
  K_grid = c(1L, 2L, 3L),
  gate_grid = c("const", "glm"),
  var_grid = c("by_component", "shared"),
  init_grid = c("kmeansA", "random"),
  max_iter = 200L,
  tol = 1e-06,
  min_sigma = 1e-06,
  min_pi = 1e-10,
  strip_fit = TRUE,
  seed = NULL
)
}
\arguments{
\item{rhs_list}{A list of RHS specifications, either as one-sided formulas
(for example, \code{~ x1 + x2}) or as character strings
(for example, \code{"x1 + x2"}). These RHS are used to build the design
matrix for the component mean regressions and, when \code{gate_grid} includes
\code{"glm"}, the gating model for \eqn{\pi_k(W)}.}

\item{K_grid}{Integer vector of candidate mixture sizes \eqn{K}.}

\item{gate_grid}{Character vector specifying gating model choices for
\eqn{\pi_k(W)}. Supported values are \code{"const"} and \code{"glm"}.}

\item{var_grid}{Character vector specifying the scale structure.
Supported values are \code{"by_component"} (component-specific \eqn{\sigma_k})
and \code{"shared"} (a common \eqn{\sigma} for all components).}

\item{init_grid}{Character vector specifying initialization strategies for
responsibilities. Supported values are \code{"kmeansA"} (k-means clustering
on \eqn{A}) and \code{"random"} (random component assignment).}

\item{max_iter}{Integer maximum number of EM iterations for each tuning row.}

\item{tol}{Nonnegative convergence tolerance for relative change in the
observed-data log-likelihood.}

\item{min_sigma}{Small positive constant used as a floor for the estimated
scale parameter(s) \eqn{\sigma} or \eqn{\sigma_k}.}

\item{min_pi}{Small positive constant used as a floor for mixture weights
\eqn{\pi_k(W)} prior to renormalization.}

\item{strip_fit}{Logical. If TRUE (default), store lightweight
coefficient-based representations sufficient for prediction and sampling.
If FALSE, store the full internal fit objects produced during EM.}

\item{seed}{Optional integer seed. If provided, each tuning row is fit with
a deterministic seed offset (for example, \code{seed + .tune}) to improve
reproducibility.}
}
\value{
A named list (runner) with elements:
  method: Character string \code{"gmm"}.
  tune_grid: Data frame describing the tuning grid, including \code{.tune}.
  fit: Function \code{fit(train_set, ...)} returning a fit bundle.
  fit_one: Function \code{fit_one(train_set, tune, ...)} fitting only the
    selected tuning index.
  log_density: Function \code{log_density(fit_bundle, newdata, ...)} returning
    an \eqn{n \times K} matrix of log-densities.
  sample: Function \code{sample(fit_bundle, newdata, n_samp, ...)} drawing
    samples (assumes \eqn{K = 1}).

Data requirements

The runner expects \code{train_set} and \code{newdata} in wide format containing:
\itemize{
\item a numeric outcome column \code{A} (required for \code{fit()} and \code{log_density()}),
\item covariates referenced in \code{rhs_list}.
}
\code{sample()} expects \code{newdata} to contain only covariates \code{W}
(it must not require an \code{A} column).
}
\description{
Constructs a runner (learner adapter) compatible with the
dsldensify() / run_direct_setting() / summarize_and_select() workflow for
direct conditional density estimation of a continuous outcome \eqn{A} given
covariates \eqn{W}.
}
\details{
This runner models the conditional density as a finite Gaussian mixture
with component-specific mean functions and either shared or component-specific
scales:
\deqn{f(a \mid W) = \sum_{k=1}^K \pi_k(W)\,\phi\!\left(a;\mu_k(W),\sigma_k\right),}
where \eqn{\phi(\cdot;\mu,\sigma)} denotes the Normal density with mean \eqn{\mu}
and standard deviation \eqn{\sigma}, and mixture weights \eqn{\pi_k(W)} satisfy
\eqn{\pi_k(W) \ge 0} and \eqn{\sum_{k=1}^K \pi_k(W) = 1}.

Component means

Each component mean is modeled as a linear regression on the design matrix
induced by \code{rhs_list}:
\deqn{\mu_k(W) = X(W)\,\beta_k,}
where \eqn{X(W)} is the model matrix produced by \code{stats::model.matrix()}
from the RHS specification and \eqn{\beta_k} is a component-specific coefficient
vector estimated by weighted least squares in the M-step of EM.

Mixture weights (gating)

The mixture weights \eqn{\pi_k(W)} are controlled by \code{gate_grid}:
\itemize{
\item \code{"const"}: constant mixing proportions, \eqn{\pi_k(W) = \pi_k}.
\item \code{"glm"}: multinomial logistic gating on the design matrix \eqn{X(W)}
  with a baseline-category parameterization and softmax normalization:
  \deqn{\pi_k(W) = \frac{\exp\{\eta_k(W)\}}{\sum_{\ell=1}^K \exp\{\eta_\ell(W)\}},}
  where \eqn{\eta_k(W) = X(W)^\top b_k} for \eqn{k=1,\dots,K-1} and
  \eqn{\eta_K(W) = 0}.
}

Scale structure

The component standard deviations are controlled by \code{var_grid}:
\itemize{
\item \code{"shared"}: a common \eqn{\sigma} is used for all components
  (\eqn{\sigma_k \equiv \sigma}).
\item \code{"by_component"}: component-specific \eqn{\sigma_k} are used.
}
In both cases, scales are floored by \code{min_sigma} for numerical stability.

Estimation by EM

For each tuning configuration, parameters are estimated by an
expectation-maximization (EM) algorithm on the observed-data log-likelihood.
Let \eqn{r_{ik}} denote the responsibility of component \eqn{k} for observation
\eqn{i}. The EM iterations proceed as follows:

E-step:
\deqn{r_{ik} \propto \pi_k(W_i)\,\phi\!\left(A_i;\mu_k(W_i),\sigma_k\right),}
with normalization so that \eqn{\sum_{k=1}^K r_{ik} = 1}.

M-step:
\enumerate{
\item Update regression coefficients \eqn{\beta_k} by weighted least squares of
  \eqn{A} on \eqn{X(W)} using weights \eqn{r_{ik}}.
\item Update \eqn{\sigma} or \eqn{\sigma_k} from weighted residual sums of squares,
  depending on \code{var_grid}.
\item Update mixture weights \eqn{\pi_k(W)} using either constant proportions
  (weighted averages of responsibilities) or multinomial logistic gating on
  \eqn{X(W)} (Newton/IRLS updates on the multinomial log-likelihood).
}

Convergence is assessed using the relative change in the observed-data log-likelihood,
stopping when the criterion falls below \code{tol} or after \code{max_iter} iterations.

Model selection via log_density()

Model selection uses likelihood-based scoring via log_density(): for each
tuning row and each observation, log_density() evaluates
\deqn{\log \hat f(A_i \mid W_i),}
where \eqn{\hat f} is the fitted Gaussian mixture density.
During cross-validation, \code{log_density()} returns an \eqn{n \times K}
matrix of log-densities aligned to \code{tune_grid$.tune}, where
\eqn{K = nrow(tune_grid)}.

Sampling from the fitted model

The runner provides a \code{sample()} method that generates draws
\deqn{A^\ast \sim \hat f(\cdot \mid W)}
by first sampling a component label \eqn{Z \in \{1,\dots,K\}} from
\eqn{\hat\pi(W)} and then sampling from the corresponding Normal distribution:
\deqn{A^\ast = \hat\mu_Z(W) + \hat\sigma_Z\,\xi,}
where \eqn{\xi \sim N(0,1)}. Sampling assumes the fit bundle contains exactly
one tuned fit (length(fit_bundle$fits) == 1), which is the intended
post-selection usage.

Numeric-only requirement

This runner is intended for use with numeric predictors only. All variables
referenced in \code{rhs_list} must be numeric. Factor handling is not
supported and variables are not coerced internally.

Tuning grid

The tuning grid is the Cartesian product of:
\itemize{
\item RHS specifications from \code{rhs_list},
\item mixture size \code{K_grid},
\item gating model choice \code{gate_grid},
\item variance structure \code{var_grid},
\item initialization strategy \code{init_grid}.
}

Stabilization and numerical safety

Mixture weights are bounded below by \code{min_pi} and renormalized to sum to one.
Scale parameters are bounded below by \code{min_sigma}. These stabilizations are
applied both during EM updates and during prediction. The internal softmax
computations for gating and the component mixture evaluations use log-sum-exp
stabilization to reduce underflow.
}
\examples{
runner <- make_gmm_runner(
  rhs_list = list(~ x1 + x2),
  K_grid = c(1L, 2L),
  gate_grid = c("const", "glm"),
  var_grid = c("shared"),
  init_grid = c("kmeansA"),
  max_iter = 150L,
  tol = 1e-6,
  min_sigma = 1e-4,
  strip_fit = TRUE,
  seed = 123
)

}
