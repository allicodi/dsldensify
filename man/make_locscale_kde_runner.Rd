% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/runner_locscale_kde.R
\name{make_locscale_kde_runner}
\alias{make_locscale_kde_runner}
\title{Create a location-scale residual KDE runner for direct conditional density estimation}
\usage{
make_locscale_kde_runner(
  rhs_list,
  mean_methods = c("glm", "glmnet", "ranger", "xgboost"),
  mean_levels = c("low", "high"),
  scale_methods = c("glm", "glmnet", "ranger", "xgboost"),
  scale_levels = c("low"),
  glmnet_alpha_grid = c(0, 1),
  glmnet_lambda_choice = c("1se", "min"),
  xgb_nrounds_low = 300L,
  xgb_nrounds_high = 800L,
  xgb_max_depth_low = 2L,
  xgb_max_depth_high = 4L,
  xgb_eta = 0.05,
  xgb_subsample = 0.8,
  xgb_colsample_bytree = 0.8,
  ranger_num_trees = 500L,
  ranger_min_node_size_low = 50L,
  ranger_min_node_size_high = 10L,
  ranger_mtry = NULL,
  kde_bw_method_grid = c("nrd0"),
  kde_adjust_grid = c(0.8, 1, 1.2),
  resid_c = 1e-08,
  sigma_floor_frac = 0.001,
  eps = 1e-12,
  use_weights_col = TRUE,
  weights_col = "wts",
  standardize_glmnet = TRUE,
  strip_fit = TRUE,
  seed = NULL,
  ...
)
}
\arguments{
\item{rhs_list}{A list of RHS specifications, either as one-sided formulas
(for example, \code{~ x1 + x2}) or as character strings
(for example, \code{"x1 + x2"}). These RHS are used to build the design
matrix for both the mean and scale models.}

\item{mean_methods}{Character vector specifying candidate algorithms for
the mean model \eqn{\mu(W)}. Supported values are \code{"glm"},
\code{"glmnet"}, \code{"ranger"}, and \code{"xgboost"}.}

\item{mean_levels}{Character vector of complexity labels used when
\code{mean_methods} includes \code{"ranger"} or \code{"xgboost"}.
Typically \code{c("low","high")}. Ignored for \code{"glm"} and \code{"glmnet"}.}

\item{scale_methods}{Character vector specifying candidate algorithms for
the scale model for \eqn{\log\{\sigma^2(W)\}}. Supported values are
\code{"glm"}, \code{"glmnet"}, \code{"ranger"}, and \code{"xgboost"}.}

\item{scale_levels}{Character vector of complexity labels used when
\code{scale_methods} includes \code{"ranger"} or \code{"xgboost"}.}

\item{glmnet_alpha_grid}{Numeric vector of \code{alpha} values used when
\code{"glmnet"} is included in \code{mean_methods} and/or \code{scale_methods}.
\code{alpha = 0} corresponds to ridge regression and \code{alpha = 1} to lasso.}

\item{glmnet_lambda_choice}{Character vector of choices for the \code{lambda}
used for prediction in \code{cv.glmnet()}. Supported values are \code{"min"}
and \code{"1se"}.}

\item{xgb_nrounds_low, xgb_nrounds_high}{Integer numbers of boosting iterations
used for \code{"xgboost"} at complexity levels \code{"low"} and \code{"high"}.}

\item{xgb_max_depth_low, xgb_max_depth_high}{Integer tree depths used for
\code{"xgboost"} at complexity levels \code{"low"} and \code{"high"}.}

\item{xgb_eta, xgb_subsample, xgb_colsample_bytree}{Tuning parameters forwarded
to \code{xgboost::xgb.train()} for \code{"xgboost"} fits.}

\item{ranger_num_trees}{Integer number of trees for \code{ranger::ranger()}.}

\item{ranger_min_node_size_low, ranger_min_node_size_high}{Integer minimum node
sizes used for \code{"ranger"} at complexity levels \code{"low"} and \code{"high"}.}

\item{ranger_mtry}{Optional integer specifying \code{mtry} for \code{"ranger"}.
If NULL, a default \eqn{\lfloor \sqrt{p} \rfloor} heuristic is used, where
\eqn{p} is the number of non-intercept predictors in the design matrix.}

\item{kde_bw_method_grid}{Character vector of KDE bandwidth rules passed to
\code{stats::density(bw = ...)}. Supported values are \code{"nrd0"} and \code{"SJ"}.}

\item{kde_adjust_grid}{Numeric vector of multiplicative bandwidth adjustments
passed to \code{stats::density(adjust = ...)}.}

\item{resid_c}{Positive constant \eqn{c} used in the scale pseudo-outcome
\eqn{\log\{(A-\hat\mu(W))^2 + c\}}.}

\item{sigma_floor_frac}{Nonnegative scalar controlling the floor applied to
predicted scales, as a fraction of \code{sd(A)}.}

\item{eps}{Small positive constant used to bound densities away from zero and
log-densities away from \eqn{-\infty}.}

\item{use_weights_col}{Logical. If TRUE and \code{train_set} contains a column
named \code{weights_col}, it is passed as case weights to supported learners
(glm, glmnet, ranger, xgboost) and to KDE when supported by
\code{stats::density()}.}

\item{weights_col}{Character name of the weight column in \code{train_set}.}

\item{standardize_glmnet}{Logical. Passed to \code{glmnet::cv.glmnet()}.}

\item{strip_fit}{Logical. If TRUE (default), store lightweight
coefficient-based representations for \code{"glm"} and \code{"glmnet"} fits.
If FALSE, store full fitted objects for these methods. Full objects are
always stored for \code{"ranger"} and \code{"xgboost"}.}

\item{seed}{Optional integer seed. If provided, each tuning row is fit with
\code{set.seed(seed + .tune)} for reproducibility.}

\item{...}{Additional arguments forwarded to the underlying fitting routines:
\code{stats::lm.fit()}, \code{glmnet::cv.glmnet()}, \code{ranger::ranger()},
and \code{xgboost::xgb.train()}.}
}
\value{
A named list (runner) with elements:
  method: Character string \code{"locscale_kde"}.
  tune_grid: Data frame describing the tuning grid, including \code{.tune}.
  fit: Function \code{fit(train_set, ...)} returning a fit bundle.
  log_density: Function \code{log_density(fit_bundle, newdata, ...)} returning
    an \eqn{n \times K} matrix of log-densities.
  density: Function \code{density(fit_bundle, newdata, ...)} returning densities.
  fit_one: Function \code{fit_one(train_set, tune, ...)} fitting only the
    selected tuning index.
  select_fit: Function \code{select_fit(fit_bundle, tune)} extracting a single
    tuning configuration.
  sample: Function \code{sample(fit_bundle, newdata, n_samp, ...)} drawing
    samples (assumes \eqn{K = 1}).

Data requirements

The runner expects \code{train_set} and \code{newdata} in wide format containing:
\itemize{
\item a numeric outcome column \code{A} (required for \code{fit()} and \code{log_density()}),
\item covariates referenced in \code{rhs_list},
\item an optional weight column named by \code{weights_col}.
}

\code{sample()} expects \code{newdata} to contain only covariates \code{W}
(it must not require an \code{A} column).
}
\description{
Constructs a runner (learner adapter) compatible with the
dsldensify() / run_direct_setting() / summarize_and_select() workflow for
direct conditional density estimation of a continuous outcome \eqn{A} given
covariates \eqn{W}.
}
\details{
The runner represents the conditional distribution using a location-scale
decomposition
\deqn{A = \mu(W) + \sigma(W)\,\varepsilon,}
where \eqn{\mu(W) = E(A \mid W)} is a conditional mean function,
\eqn{\sigma(W) > 0} is a conditional scale function, and \eqn{\varepsilon}
is a standardized residual with density \eqn{g}.

Estimation proceeds in three stages for each tuning configuration:
\enumerate{
\item Fit a mean model \eqn{\hat\mu(W)}.
\item Fit a scale model for \eqn{\log\{\sigma^2(W)\}} using the pseudo-outcome
  \eqn{\log\{(A - \hat\mu(W))^2 + c\}}, where \eqn{c > 0} is a stabilizing constant.
\item Form standardized residuals \eqn{\hat\varepsilon = (A - \hat\mu(W))/\hat\sigma(W)}
  and estimate \eqn{g} by a univariate kernel density estimate on the residual scale.
}

The implied conditional density estimator is
\deqn{\hat f(a \mid W) = \hat g\!\left(\frac{a - \hat\mu(W)}{\hat\sigma(W)}\right)\,\frac{1}{\hat\sigma(W)}.}

Model selection uses likelihood-based scoring via log_density(): for each
tuning row and each observation, log_density() evaluates \eqn{\log \hat f(A_i \mid W_i)}.

Mean and scale learners

The runner supports multiple algorithms for the mean and scale models:
\itemize{
\item \code{"glm"}: least squares via \code{stats::lm.fit()}.
\item \code{"glmnet"}: penalized least squares via \code{glmnet::cv.glmnet()}.
\item \code{"ranger"}: random forest regression via \code{ranger::ranger()}.
\item \code{"xgboost"}: boosted trees via \code{xgboost::xgb.train()}.
}

For \code{"ranger"} and \code{"xgboost"}, a coarse complexity label
(\code{"low"} or \code{"high"}) may be included as part of tuning to provide
small, fixed grids without exposing many low-level hyperparameters.
For \code{"glmnet"}, tuning uses \code{alpha} and a choice of \code{lambda}
(\code{"min"} or \code{"1se"}).

Kernel density estimation on residuals

The residual density \eqn{g} is estimated using \code{stats::density()} on the
training residuals within each tuning configuration. The KDE bandwidth is
controlled by \code{kde_bw_method_grid} and \code{kde_adjust_grid}. The KDE is
stored as a linear interpolant \eqn{\hat g} via \code{approxfun()}.

Sampling from the fitted model

The runner provides a \code{sample()} method that generates draws
\deqn{A^\ast \sim \hat f(\cdot \mid W)}
via
\deqn{A^\ast = \hat\mu(W) + \hat\sigma(W)\,\varepsilon^\ast,}
where \eqn{\varepsilon^\ast} is obtained by resampling training residuals
(optionally weighted) and applying Gaussian jitter with standard deviation
equal to the effective KDE bandwidth. Sampling assumes the fit bundle contains
exactly one tuned fit (length(fit_bundle$fits) == 1), which is the intended
post-selection usage.

Numeric-only requirement

This runner is intended for use with numeric predictors only. All variables
referenced in \code{rhs_list} must be numeric. Factor handling is not
supported and variables are not coerced internally.

Tuning grid and prediction layout

The tuning grid is the Cartesian product of:
\itemize{
\item RHS specifications from \code{rhs_list},
\item mean model specification (method and method-specific tuning),
\item scale model specification (method and method-specific tuning),
\item KDE bandwidth method and adjustment.
}

During cross-validation, \code{log_density()} returns an \eqn{n \times K}
matrix of log-densities aligned to \code{tune_grid$.tune}, where
\eqn{K = nrow(tune_grid)}.

Lightweight fit objects

When \code{strip_fit = TRUE} (default), \code{"glm"} and \code{"glmnet"} fits
are reduced to coefficient-based representations sufficient for prediction:
\itemize{
\item \code{"glm"}: coefficient vector aligned to the training design columns.
\item \code{"glmnet"}: coefficient vector at the selected \code{lambda}
  (either \code{"min"} or \code{"1se"}).
}

When \code{strip_fit = FALSE}, full fitted objects are stored. For
\code{"ranger"} and \code{"xgboost"}, full fitted objects are always stored.

Stabilization and numerical safety

The scale model is fit to \eqn{\log\{(A-\hat\mu(W))^2 + c\}} with
\code{resid_c = c}. Predicted scales are floored by
\eqn{\max\{\sigma_{\min}, \hat\sigma(W)\}} where
\eqn{\sigma_{\min} = \max\{\mathrm{sigma\_floor\_frac} \cdot \mathrm{sd}(A), \mathrm{eps}\}}.
Density values are bounded below by \code{eps} before taking logs.
}
\examples{
runner <- make_locscale_kde_runner(
  rhs_list = list(~ x1 + x2),
  mean_methods = c("glm", "glmnet", "ranger", "xgboost"),
  mean_levels = c("low"),
  scale_methods = c("glm", "glmnet"),
  scale_levels = c("low"),
  glmnet_alpha_grid = c(0, 1),
  glmnet_lambda_choice = c("1se"),
  kde_bw_method_grid = c("nrd0"),
  kde_adjust_grid = c(1.0),
  strip_fit = TRUE,
  eps = 1e-10,
  seed = 123
)

}
