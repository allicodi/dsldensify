% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{run_direct_setting}
\alias{run_direct_setting}
\title{Evaluate direct density learners for a single cross-validation setting}
\usage{
run_direct_setting(
  A,
  W,
  wts,
  cv_folds_id,
  id_fold,
  learners,
  return_fits = TRUE,
  return_density = FALSE,
  eps = 1e-15,
  ...
)
}
\arguments{
\item{A}{Numeric vector of length \code{n} containing observed outcomes.}

\item{W}{Covariates used to condition the density. May be a vector, matrix,
\code{data.frame}, or \code{data.table}.}

\item{wts}{Numeric vector of length \code{n} giving observation weights.}

\item{cv_folds_id}{Fold object as returned by \code{origami::make_folds()}.
Must have length \code{V}, with each element containing a
\code{validation_set} index vector.}

\item{id_fold}{Integer vector of length \code{n} giving the fold assignment
for each observation, with values in \code{1, ..., V}. Typically constructed
from \code{cv_folds_id}.}

\item{learners}{Named list of direct density runners. Each runner must define
a \code{fit(train_set, ...)} method and a
\code{log_density(fit_bundle, newdata, ...)} method. The \code{fit} method
is called on the wide training data, and \code{log_density} is called on the
wide validation data.}

\item{return_fits}{Logical; whether to store each fitted learner object for
each fold in the returned structure. Storing fits can be memory-intensive.}

\item{return_density}{Logical; whether to additionally store the validation
density values \eqn{f(A \mid W)} (obtained by exponentiating the log-density)
for each fold and learner.}

\item{eps}{Small positive constant passed to \code{runner$log_density()} to
bound log-density evaluations away from \code{-Inf} when supported.}

\item{...}{Additional arguments passed to \code{runner$fit()} and
\code{runner$log_density()}.}
}
\value{
A named list with components:
\describe{
  \item{cv_out}{List of length \code{V}. Each element contains the fold index,
    a fold weight (sum of validation weights), and a named list of learner
    results. Each learner result contains a loss matrix of dimension
    \code{n_valid} by \code{K}, where \code{K} is the number of tuning
    configurations for that learner (or 1 if untuned).}
  \item{breaks}{\code{NULL}. Included for compatibility with hazard-based
    outputs.}
  \item{bin_length}{\code{NULL}. Included for compatibility with hazard-based
    outputs.}
  \item{grid_type}{Character string \code{"direct"}.}
  \item{n_bins}{\code{NA_integer_}. Included for compatibility with hazard-based
    outputs.}
}
}
\description{
Runs cross-validation for a collection of direct conditional density learners
(runners) on wide-format data. Each learner is trained on the training split
for each fold and evaluated on the validation split using negative
log-density loss, \eqn{-\log f(A \mid W)}.
}
\details{
This function is used internally by \code{\link{dsldensify}} to evaluate
direct density learners once (no binning grid), producing fold-level outputs
in a standardized format compatible with \code{summarize_and_select()}.
}
